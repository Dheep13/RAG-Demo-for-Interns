{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Comprehensive RAG Demo - All Methods\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **three different RAG approaches** in a progressive learning format:\n",
    "\n",
    "1. **ğŸ“š Basic Text RAG**: Simple embedding with sample company data\n",
    "2. **ğŸ“„ Document RAG**: Processing real PDF and Word documents\n",
    "3. **ğŸ–¼ï¸ Multi-Modal RAG**: Text + Images using GPT-4o Vision\n",
    "\n",
    "Perfect for intern training and understanding RAG evolution!\n",
    "\n",
    "### What You'll Learn:\n",
    "- âœ… Core RAG concepts and implementation\n",
    "- âœ… Document processing techniques\n",
    "- âœ… Multi-modal AI capabilities\n",
    "- âœ… Real-world applications\n",
    "- âœ… Latest OpenAI Vision API usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-chroma langchain-community\n",
    "!pip install tiktoken chromadb pypdf docx2txt PyMuPDF\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded from .env file\n",
      "âœ… OpenAI API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "def load_env_file():\n",
    "    \"\"\"Load environment variables from .env file\"\"\"\n",
    "    code_snippets_path = Path(\"Code Snippets\")\n",
    "    env_file = code_snippets_path / \".env\"\n",
    "    \n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if '=' in line and not line.startswith('#'):\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    value = value.strip(\"'\\\"\")\n",
    "                    os.environ[key] = value\n",
    "        print(\"âœ… Environment variables loaded from .env file\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âŒ .env file not found\")\n",
    "        return False\n",
    "\n",
    "# Load the environment variables\n",
    "load_env_file()\n",
    "\n",
    "# Check if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"âŒ OpenAI API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ ComprehensiveRAGDemo Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ComprehensiveRAGDemo class defined!\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveRAGDemo:\n",
    "    \"\"\"\n",
    "    Comprehensive RAG implementation with multiple approaches:\n",
    "    1. Basic text RAG with sample data\n",
    "    2. Document RAG with PDF/Word files\n",
    "    3. Multi-modal RAG with text + images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize comprehensive RAG system.\"\"\"\n",
    "        print(\"ğŸš€ Initializing Comprehensive RAG Demo...\")\n",
    "        \n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        # Use the latest vision model\n",
    "        self.vision_llm = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "        \n",
    "        # Text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "        self.processed_content = []\n",
    "        \n",
    "        print(\"âœ… Comprehensive RAG Demo initialized!\")\n",
    "        print(\"ğŸ”§ Supports: Basic Text, Document Processing, Multi-Modal\")\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question using the current RAG setup.\"\"\"\n",
    "        if self.qa_chain is None:\n",
    "            raise ValueError(\"No knowledge base created!\")\n",
    "        \n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"source_documents\": result[\"source_documents\"]\n",
    "        }\n",
    "\n",
    "print(\"âœ… ComprehensiveRAGDemo class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Comprehensive RAG Demo...\n",
      "âœ… Comprehensive RAG Demo initialized!\n",
      "ğŸ”§ Supports: Basic Text, Document Processing, Multi-Modal\n",
      "ğŸ¯ RAG system ready with:\n",
      "   ğŸ“Š OpenAI Embeddings (ada-002)\n",
      "   ğŸ¤– GPT-3.5-turbo for text generation\n",
      "   ğŸ‘ï¸ GPT-4o for vision analysis\n"
     ]
    }
   ],
   "source": [
    "# Initialize the comprehensive RAG demo\n",
    "rag_demo = ComprehensiveRAGDemo()\n",
    "\n",
    "print(\"ğŸ¯ RAG system ready with:\")\n",
    "print(\"   ğŸ“Š OpenAI Embeddings (ada-002)\")\n",
    "print(\"   ğŸ¤– GPT-3.5-turbo for text generation\")\n",
    "print(\"   ğŸ‘ï¸ GPT-4o for vision analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š SECTION 1: Basic Text RAG\n",
    "\n",
    "Let's start with the fundamentals - basic text embedding and retrieval using sample company data.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand document chunking\n",
    "- See embedding creation\n",
    "- Experience vector similarity search\n",
    "- Learn answer generation with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Basic Text RAG methods added with embedding visualization!\n"
     ]
    }
   ],
   "source": [
    "# Add Basic Text RAG methods to our class\n",
    "def demo_basic_text_rag(self):\n",
    "    \"\"\"Demonstrate basic RAG with sample text data.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“š SECTION 1: BASIC TEXT RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample company documents\n",
    "    sample_docs = [\n",
    "        \"\"\"\n",
    "        Company Policy: Remote Work Guidelines\n",
    "        \n",
    "        Our company supports flexible remote work arrangements. Employees can work from home \n",
    "        up to 3 days per week with manager approval. Remote work days must be scheduled in advance.\n",
    "        \n",
    "        Equipment: The company provides laptops and necessary software for remote work.\n",
    "        Communication: Daily check-ins via Slack are required for remote workers.\n",
    "        Productivity: Remote workers must maintain the same productivity standards as office workers.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Employee Benefits Overview\n",
    "        \n",
    "        Health Insurance: Full medical, dental, and vision coverage provided.\n",
    "        Vacation Policy: 20 days of paid vacation per year, plus 10 sick days.\n",
    "        Professional Development: $2000 annual budget for training and conferences.\n",
    "        Retirement: 401k with 4% company matching.\n",
    "        Wellness: Free gym membership and mental health support.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        IT Security Guidelines\n",
    "        \n",
    "        Password Requirements: Minimum 12 characters with special characters.\n",
    "        VPN: Required for all remote connections to company systems.\n",
    "        Software Updates: Automatic updates must be enabled on all devices.\n",
    "        Data Protection: No company data on personal devices without encryption.\n",
    "        Incident Reporting: Security incidents must be reported within 1 hour.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Meeting Room Booking System\n",
    "        \n",
    "        Conference rooms can be booked through the company portal.\n",
    "        Maximum booking duration: 4 hours per session.\n",
    "        Cancellation: Must cancel at least 2 hours in advance.\n",
    "        Equipment: All rooms have projectors, whiteboards, and video conferencing.\n",
    "        Catering: Can be arranged through HR for meetings over 2 hours.\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“„ Processing {len(sample_docs)} sample documents...\")\n",
    "    \n",
    "    # Convert to Document objects\n",
    "    documents = [Document(page_content=doc) for doc in sample_docs]\n",
    "    \n",
    "    # Create knowledge base\n",
    "    self._create_basic_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions\n",
    "    demo_questions = [\n",
    "        \"How many days can I work from home?\",\n",
    "        \"What's our vacation policy?\",\n",
    "        \"What are the password requirements?\",\n",
    "        \"How do I book a meeting room?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸª Basic RAG Demo Questions:\")\n",
    "    for i, question in enumerate(demo_questions, 1):\n",
    "        print(f\"\\n{i}. {question}\")\n",
    "        result = self.query(question)\n",
    "        print(f\"ğŸ’¡ Answer: {result['answer']}\")\n",
    "        print(f\"ğŸ“– Sources: {len(result['source_documents'])} chunks\")\n",
    "    \n",
    "    print(\"\\nâœ… Basic Text RAG demonstration complete!\")\n",
    "\n",
    "def _create_basic_knowledge_base(self, documents):\n",
    "    \"\"\"Create knowledge base from basic documents.\"\"\"\n",
    "    # Split documents\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"ğŸ”ª Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Basic knowledge base created!\")\n",
    "    \n",
    "    # Show embedding details for educational purposes\n",
    "    self._show_embedding_details(chunks[:2])  # Show first 2 chunks\n",
    "\n",
    "def _show_embedding_details(self, sample_chunks):\n",
    "    \"\"\"Show embedding details for educational purposes.\"\"\"\n",
    "    print(\"\\nğŸ” EMBEDDING DETAILS (Educational):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, chunk in enumerate(sample_chunks, 1):\n",
    "        print(f\"\\nğŸ“„ Chunk {i}:\")\n",
    "        print(f\"Text: {chunk.page_content[:100]}...\")\n",
    "        print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "        \n",
    "        # Generate embedding for this chunk\n",
    "        try:\n",
    "            embedding = self.embeddings.embed_query(chunk.page_content)\n",
    "            print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "            print(f\"Embedding type: {type(embedding)}\")\n",
    "            print(f\"First 10 values: {embedding[:10]}\")\n",
    "            print(f\"Embedding range: [{min(embedding):.4f}, {max(embedding):.4f}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Key Points:\")\n",
    "    print(\"   â€¢ Each text chunk becomes a 1536-dimensional vector\")\n",
    "    print(\"   â€¢ Similar texts have similar embeddings (cosine similarity)\")\n",
    "    print(\"   â€¢ Vector database enables fast similarity search\")\n",
    "\n",
    "def query_database_directly(self, query_text, k=3):\n",
    "    \"\"\"Query the vector database directly and show similarity scores.\"\"\"\n",
    "    if self.vectorstore is None:\n",
    "        print(\"âŒ No vector database available. Create knowledge base first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ” DIRECT DATABASE QUERY: '{query_text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = self.embeddings.embed_query(query_text)\n",
    "    print(f\"Query embedding dimensions: {len(query_embedding)}\")\n",
    "    print(f\"Query embedding preview: {query_embedding[:5]}...\")\n",
    "    \n",
    "    # Search with similarity scores\n",
    "    try:\n",
    "        # Use similarity_search_with_score for detailed results\n",
    "        results = self.vectorstore.similarity_search_with_score(query_text, k=k)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Top {k} Similar Chunks:\")\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Similarity Score: {score:.4f}\")\n",
    "            print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            print(f\"   Type: {doc.metadata.get('type', 'Unknown')}\")\n",
    "            print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ Understanding Similarity Scores:\")\n",
    "        print(\"   â€¢ Lower scores = more similar (distance-based)\")\n",
    "        print(\"   â€¢ Scores typically range from 0.0 to 2.0\")\n",
    "        print(\"   â€¢ Score < 0.5: Very similar\")\n",
    "        print(\"   â€¢ Score 0.5-1.0: Moderately similar\")\n",
    "        print(\"   â€¢ Score > 1.0: Less similar\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error querying database: {e}\")\n",
    "        return None\n",
    "\n",
    "def show_vector_database_stats(self):\n",
    "    \"\"\"Show statistics about the vector database.\"\"\"\n",
    "    if self.vectorstore is None:\n",
    "        print(\"âŒ No vector database available.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ“Š VECTOR DATABASE STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get collection info\n",
    "        collection = self.vectorstore._collection\n",
    "        count = collection.count()\n",
    "        \n",
    "        print(f\"Total vectors stored: {count}\")\n",
    "        print(f\"Embedding model: {self.embeddings.model}\")\n",
    "        print(f\"Vector dimensions: 1536 (OpenAI ada-002)\")\n",
    "        print(f\"Database type: ChromaDB (in-memory)\")\n",
    "        \n",
    "        # Sample a few vectors to show distribution\n",
    "        if count > 0:\n",
    "            sample_docs = self.vectorstore.similarity_search(\"sample\", k=min(3, count))\n",
    "            print(f\"\\nğŸ“„ Sample stored chunks:\")\n",
    "            for i, doc in enumerate(sample_docs, 1):\n",
    "                print(f\"   {i}. {doc.page_content[:80]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database stats: {e}\")\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_basic_text_rag = demo_basic_text_rag\n",
    "ComprehensiveRAGDemo._create_basic_knowledge_base = _create_basic_knowledge_base\n",
    "ComprehensiveRAGDemo._show_embedding_details = _show_embedding_details\n",
    "ComprehensiveRAGDemo.query_database_directly = query_database_directly\n",
    "ComprehensiveRAGDemo.show_vector_database_stats = show_vector_database_stats\n",
    "\n",
    "print(\"âœ… Basic Text RAG methods added with embedding visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“š SECTION 1: BASIC TEXT RAG DEMO\n",
      "============================================================\n",
      "ğŸ“„ Processing 4 sample documents...\n",
      "ğŸ”ª Created 4 chunks\n",
      "âœ… Basic knowledge base created!\n",
      "\n",
      "ğŸ” EMBEDDING DETAILS (Educational):\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“„ Chunk 1:\n",
      "Text: Company Policy: Remote Work Guidelines\n",
      "\n",
      "        Our company supports flexible remote work arrangemen...\n",
      "Length: 506 characters\n",
      "Embedding dimensions: 1536\n",
      "Embedding type: <class 'list'>\n",
      "First 10 values: [0.001289139618165791, 0.0013582154642790556, 0.0027630364056676626, -0.0363256074488163, -0.01419468317180872, 0.025047091767191887, -0.01492705475538969, 0.00013804779155179858, -0.011764543130993843, -0.00920790247619152]\n",
      "Embedding range: [-0.6392, 0.2088]\n",
      "\n",
      "ğŸ“„ Chunk 2:\n",
      "Text: Employee Benefits Overview\n",
      "\n",
      "        Health Insurance: Full medical, dental, and vision coverage prov...\n",
      "Length: 384 characters\n",
      "Embedding dimensions: 1536\n",
      "Embedding type: <class 'list'>\n",
      "First 10 values: [0.0015996930887922645, 0.00623961491510272, 0.022749746218323708, -0.0575304850935936, -0.03830168768763542, 0.007041897624731064, -0.0026991774793714285, -0.01257017720490694, -0.004164074081927538, 0.002989883068948984]\n",
      "Embedding range: [-0.6403, 0.1959]\n",
      "\n",
      "ğŸ’¡ Key Points:\n",
      "   â€¢ Each text chunk becomes a 1536-dimensional vector\n",
      "   â€¢ Similar texts have similar embeddings (cosine similarity)\n",
      "   â€¢ Vector database enables fast similarity search\n",
      "\n",
      "ğŸª Basic RAG Demo Questions:\n",
      "\n",
      "1. How many days can I work from home?\n",
      "ğŸ’¡ Answer: You can work from home up to 3 days per week with manager approval.\n",
      "ğŸ“– Sources: 3 chunks\n",
      "\n",
      "2. What's our vacation policy?\n",
      "ğŸ’¡ Answer: Our vacation policy includes 20 days of paid vacation per year, along with 10 sick days.\n",
      "ğŸ“– Sources: 3 chunks\n",
      "\n",
      "3. What are the password requirements?\n",
      "ğŸ’¡ Answer: The password requirements are a minimum of 12 characters with special characters.\n",
      "ğŸ“– Sources: 3 chunks\n",
      "\n",
      "4. How do I book a meeting room?\n",
      "ğŸ’¡ Answer: You can book a meeting room through the company portal.\n",
      "ğŸ“– Sources: 3 chunks\n",
      "\n",
      "âœ… Basic Text RAG demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Basic Text RAG Demo\n",
    "rag_demo.demo_basic_text_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Key Takeaways from Basic RAG:\n",
    "\n",
    "1. **Document Chunking**: Text is split into manageable pieces\n",
    "2. **Embeddings**: Each chunk becomes a vector representation\n",
    "3. **Vector Search**: Find most similar chunks to user question\n",
    "4. **Context Injection**: Relevant chunks are added to LLM prompt\n",
    "5. **Answer Generation**: LLM creates response based on context\n",
    "\n",
    "**Try your own questions about the company policies!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: What is the india vs zimbabwe current score?\n",
      "ğŸ’¡ Answer: I don't know.\n",
      "ğŸ“– Sources: 3 chunks used\n"
     ]
    }
   ],
   "source": [
    "# Try your own question with basic RAG\n",
    "question = \"What is the india vs zimbabwe current score?\"\n",
    "\n",
    "result = rag_demo.query(question)\n",
    "print(f\"â“ Question: {question}\")\n",
    "print(f\"ğŸ’¡ Answer: {result['answer']}\")\n",
    "print(f\"ğŸ“– Sources: {len(result['source_documents'])} chunks used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Deep Dive: Understanding Embeddings and Vector Search\n",
    "\n",
    "Let's explore what's happening under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š VECTOR DATABASE STATISTICS:\n",
      "----------------------------------------\n",
      "Total vectors stored: 8\n",
      "Embedding model: text-embedding-ada-002\n",
      "Vector dimensions: 1536 (OpenAI ada-002)\n",
      "Database type: ChromaDB (in-memory)\n",
      "\n",
      "ğŸ“„ Sample stored chunks:\n",
      "   1. Company Policy: Remote Work Guidelines\n",
      "\n",
      "        Our company supports flexible re...\n",
      "   2. Company Policy: Remote Work Guidelines\n",
      "\n",
      "        Our company supports flexible re...\n",
      "   3. Employee Benefits Overview\n",
      "\n",
      "        Health Insurance: Full medical, dental, and ...\n"
     ]
    }
   ],
   "source": [
    "# Show vector database statistics\n",
    "rag_demo.show_vector_database_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database directly to see similarity scores\n",
    "query_text = \"remote work policy\"\n",
    "results = rag_demo.query_database_directly(query_text, k=3)\n",
    "\n",
    "print(\"\\nğŸ¯ This shows you exactly how vector similarity search works!\")\n",
    "print(\"The RAG system uses these top chunks to generate the final answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different queries and their similarity scores\n",
    "test_queries = [\n",
    "    \"vacation days\",\n",
    "    \"password security\",\n",
    "    \"meeting room booking\",\n",
    "    \"completely unrelated topic\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª SIMILARITY COMPARISON TEST:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ” Query: '{query}'\")\n",
    "    results = rag_demo.query_database_directly(query, k=1)\n",
    "    if results:\n",
    "        best_score = results[0][1]\n",
    "        print(f\"   Best similarity score: {best_score:.4f}\")\n",
    "        if best_score < 0.5:\n",
    "            print(\"   âœ… Excellent match found!\")\n",
    "        elif best_score < 1.0:\n",
    "            print(\"   âš ï¸ Moderate match found\")\n",
    "        else:\n",
    "            print(\"   âŒ Poor match - might hallucinate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Embedding Visualization Exercise\n",
    "\n",
    "Let's see how different texts create different embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings of similar vs different texts\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"Employees can work from home\",\n",
    "    \"Remote work is allowed for staff\",  # Similar meaning\n",
    "    \"The sky is blue today\",  # Completely different\n",
    "    \"Password must be 12 characters\"  # Different topic\n",
    "]\n",
    "\n",
    "print(\"ğŸ§® EMBEDDING SIMILARITY ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = rag_demo.embeddings.embed_query(text)\n",
    "    embeddings.append(embedding)\n",
    "    print(f\"\\nğŸ“ Text: '{text}'\")\n",
    "    print(f\"   Embedding dimensions: {len(embedding)}\")\n",
    "    print(f\"   First 5 values: {embedding[:5]}\")\n",
    "\n",
    "# Calculate cosine similarities\n",
    "print(\"\\nğŸ”— COSINE SIMILARITY MATRIX:\")\n",
    "print(\"(1.0 = identical, 0.0 = completely different)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i <= j:  # Only show upper triangle\n",
    "            similarity = similarity_matrix[i][j]\n",
    "            print(f\"'{text1[:20]}...' vs '{text2[:20]}...': {similarity:.3f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice how similar meanings have higher cosine similarity!\")\n",
    "print(\"This is the foundation of semantic search in RAG systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“„ SECTION 2: Document RAG\n",
    "\n",
    "Now let's process real documents from your EmbeddingDocs folder!\n",
    "\n",
    "### What's Different:\n",
    "- Real PDF and Word document processing\n",
    "- Automatic text extraction\n",
    "- Metadata preservation\n",
    "- Larger, more complex knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document RAG methods added!\n"
     ]
    }
   ],
   "source": [
    "# Add Document RAG methods to our class\n",
    "def demo_document_rag(self):\n",
    "    \"\"\"Demonstrate document RAG with PDF and Word files.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“„ SECTION 2: DOCUMENT RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load documents from folder\n",
    "    documents = self.load_documents_from_folder()\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"âŒ No documents found in EmbeddingDocs folder\")\n",
    "        return\n",
    "    \n",
    "    # Create knowledge base\n",
    "    self.create_document_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions based on loaded documents\n",
    "    print(\"\\nğŸª Document RAG Demo Questions:\")\n",
    "    \n",
    "    # Check what documents we have and ask relevant questions\n",
    "    doc_names = [doc.metadata.get('source', '') for doc in documents]\n",
    "    \n",
    "    if any('attention' in name.lower() for name in doc_names):\n",
    "        print(\"\\nğŸ“„ Questions about 'Attention is All You Need' paper:\")\n",
    "        questions = [\n",
    "            \"What is the Transformer architecture?\",\n",
    "            \"How does self-attention work?\",\n",
    "            \"What are the advantages of Transformers over RNNs?\",\n",
    "            \"Of all the references mentioned in the paper, can you get me the 14th reference(Zhongqiang Huang and Mary Harper)?\"\n",
    "        ]\n",
    "        for q in questions:\n",
    "            result = self.query(q)\n",
    "            print(f\"â“ {q}\")\n",
    "            print(f\"ğŸ’¡ {result['answer'][:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    if any('boomi' in name.lower() for name in doc_names):\n",
    "        print(\"\\nğŸ“„ Questions about Boomi document:\")\n",
    "        questions = [\n",
    "            \"What is Boomi used for?\",\n",
    "            \"How does Boomi integration work?\",\n",
    "            \" What is mentioned under the heading - Maxxton Reservation to Salesforce?\"\n",
    "        ]\n",
    "        for q in questions:\n",
    "            result = self.query(q)\n",
    "            print(f\"â“ {q}\")\n",
    "            print(f\"ğŸ’¡ {result['answer'][:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    print(\"âœ… Document RAG demonstration complete!\")\n",
    "\n",
    "def load_pdf_document(self, file_path):\n",
    "    \"\"\"Load a PDF document (text only).\"\"\"\n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        \n",
    "        print(f\"ğŸ“„ Loading PDF: {file_path.name}\")\n",
    "        loader = PyPDFLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        print(f\"   âœ… Loaded {len(documents)} pages\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_word_document(self, file_path):\n",
    "    \"\"\"Load a Word document.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.document_loaders import Docx2txtLoader\n",
    "        \n",
    "        print(f\"ğŸ“„ Loading Word document: {file_path.name}\")\n",
    "        loader = Docx2txtLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"Word_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        print(f\"   âœ… Loaded Word document\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Word document: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_from_folder(self, folder_path=\"EmbeddingDocs\"):\n",
    "    \"\"\"Load all documents from the specified folder.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"âŒ Folder {folder_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ğŸ“ Loading documents from: {folder}\")\n",
    "    all_documents = []\n",
    "    \n",
    "    # Find files\n",
    "    pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "    word_files = list(folder.glob(\"*.docx\")) + list(folder.glob(\"*.doc\"))\n",
    "    \n",
    "    print(f\"   Found {len(pdf_files)} PDF files and {len(word_files)} Word files\")\n",
    "    \n",
    "    # Process PDF files\n",
    "    for pdf_file in pdf_files:\n",
    "        docs = self.load_pdf_document(pdf_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    # Process Word files\n",
    "    for word_file in word_files:\n",
    "        docs = self.load_word_document(word_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    self.processed_content = all_documents\n",
    "    print(f\"ğŸ“š Total documents loaded: {len(all_documents)}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def create_document_knowledge_base(self, documents):\n",
    "    \"\"\"Create knowledge base from documents.\"\"\"\n",
    "    print(f\"ğŸ”ª Processing {len(documents)} documents...\")\n",
    "    \n",
    "    # Split documents\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"ğŸ“„ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Document knowledge base created!\")\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_document_rag = demo_document_rag\n",
    "ComprehensiveRAGDemo.load_pdf_document = load_pdf_document\n",
    "ComprehensiveRAGDemo.load_word_document = load_word_document\n",
    "ComprehensiveRAGDemo.load_documents_from_folder = load_documents_from_folder\n",
    "ComprehensiveRAGDemo.create_document_knowledge_base = create_document_knowledge_base\n",
    "\n",
    "print(\"âœ… Document RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“„ SECTION 2: DOCUMENT RAG DEMO\n",
      "============================================================\n",
      "ğŸ“ Loading documents from: EmbeddingDocs\n",
      "   Found 1 PDF files and 1 Word files\n",
      "ğŸ“„ Loading PDF: Attention is All You Need.pdf\n",
      "   âœ… Loaded 15 pages\n",
      "ğŸ“„ Loading Word document: sample boomi dd (1).docx\n",
      "   âœ… Loaded Word document\n",
      "ğŸ“š Total documents loaded: 16\n",
      "ğŸ”ª Processing 16 documents...\n",
      "ğŸ“„ Created 64 chunks\n",
      "âœ… Document knowledge base created!\n",
      "\n",
      "ğŸª Document RAG Demo Questions:\n",
      "\n",
      "ğŸ“„ Questions about 'Attention is All You Need' paper:\n",
      "â“ What is the Transformer architecture?\n",
      "ğŸ’¡ The Transformer architecture is a model that relies entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence. It allows for more parallelization an...\n",
      "\n",
      "â“ How does self-attention work?\n",
      "ğŸ’¡ Self-attention is an attention mechanism that relates different positions of a single sequence to compute a representation of that sequence. It allows the model to weigh the importance of different wo...\n",
      "\n",
      "â“ What are the advantages of Transformers over RNNs?\n",
      "ğŸ’¡ The advantages of Transformers over RNNs include the ability to compute representations of input and output using self-attention without relying on sequence-aligned RNNs or convolution. Transformers c...\n",
      "\n",
      "â“ Of all the references mentioned in the paper, can you get me the 14th reference(Zhongqiang Huang and Mary Harper)?\n",
      "ğŸ’¡ I'm sorry, but the references you provided do not include a 14th reference by Zhongqiang Huang and Mary Harper....\n",
      "\n",
      "\n",
      "ğŸ“„ Questions about Boomi document:\n",
      "â“ What is Boomi used for?\n",
      "ğŸ’¡ Boomi is used for integration processes, specifically for syncing data between different systems. It helps in connecting applications, data sources, and devices to enable the seamless flow of informat...\n",
      "\n",
      "â“ How does Boomi integration work?\n",
      "ğŸ’¡ Boomi integration works by connecting different applications, data sources, and systems to enable seamless data flow and communication between them. Boomi uses a visual interface to design integration...\n",
      "\n",
      "â“  What is mentioned under the heading - Maxxton Reservation to Salesforce?\n",
      "ğŸ’¡ Under the heading \"Maxxton Reservation to Salesforce,\" the following information is provided:\n",
      "\n",
      "- Process Name: Maxxton Reservation to Salesforce\n",
      "- Integration Type: Scheduled (Job)\n",
      "- Description: Sync...\n",
      "\n",
      "âœ… Document RAG demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Document RAG Demo\n",
    "rag_demo.demo_document_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Document RAG Insights:\n",
    "\n",
    "1. **File Format Support**: PDFs, Word docs automatically processed\n",
    "2. **Text Extraction**: Handles complex document layouts\n",
    "3. **Metadata Tracking**: Source attribution for answers\n",
    "4. **Scalability**: Can handle hundreds of documents\n",
    "5. **Real-World Ready**: Production-ready document processing\n",
    "\n",
    "**Ask questions about your actual documents!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ What is the main contribution of the Transformer architecture?\n",
      "ğŸ’¡ The main contribution of the Transformer architecture is that it relies entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence and convolutions. This approach allows for significantly more parallelization, superior quality in tasks like machine ...\n",
      "ğŸ“– Sources: 5 document chunks\n",
      "\n",
      "â“ How does attention mechanism work?\n",
      "ğŸ’¡ An attention mechanism works by mapping a query and a set of key-value pairs to an output. The query, keys, values, and output are all vectors. The output is computed as a weighted sum based on the similarity between the query and the keys. This mechanism allows the model to focus on different parts...\n",
      "ğŸ“– Sources: 5 document chunks\n",
      "\n",
      "â“ What are the key components of Boomi integration?\n",
      "ğŸ’¡ The key components of Boomi integration mentioned in the provided context are:\n",
      "\n",
      "1. Boomi API / WebService: Contains details of all APIs hosted within the Boomi integration layer, including the resource, HTTP method, Boomi endpoint, message format, and Boomi process name.\n",
      "\n",
      "2. Boomi Schedule Job Detai...\n",
      "ğŸ“– Sources: 5 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Interactive questions about your documents\n",
    "questions = [\n",
    "    \"What is the main contribution of the Transformer architecture?\",\n",
    "    \"How does attention mechanism work?\",\n",
    "    \"What are the key components of Boomi integration?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    try:\n",
    "        result = rag_demo.query(q)\n",
    "        print(f\"\\nâ“ {q}\")\n",
    "        print(f\"ğŸ’¡ {result['answer'][:300]}...\")\n",
    "        print(f\"ğŸ“– Sources: {len(result['source_documents'])} document chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ–¼ï¸ SECTION 3: Multi-Modal RAG (Text + Images)\n",
    "\n",
    "The cutting-edge approach - understanding both text AND images!\n",
    "\n",
    "### Revolutionary Capabilities:\n",
    "- ğŸ–¼ï¸ Extract images from PDFs\n",
    "- ğŸ‘ï¸ AI-powered image description using GPT-4o\n",
    "- ğŸ§  Combined text + visual understanding\n",
    "- ğŸ“Š Analyze charts, diagrams, and figures\n",
    "- ğŸ” Search across both content types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-Modal RAG methods added!\n"
     ]
    }
   ],
   "source": [
    "# Add Multi-Modal RAG methods to our class\n",
    "def demo_multimodal_rag(self):\n",
    "    \"\"\"Demonstrate multi-modal RAG with images.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ–¼ï¸ SECTION 3: MULTI-MODAL RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load documents with images\n",
    "    documents = self.load_documents_with_images()\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"âŒ No documents found in EmbeddingDocs folder\")\n",
    "        return\n",
    "    \n",
    "    # Create multi-modal knowledge base\n",
    "    self.create_multimodal_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions focusing on visual content\n",
    "    print(\"\\nğŸª Multi-Modal RAG Demo Questions:\")\n",
    "    \n",
    "    visual_questions = [\n",
    "        \"What diagrams or figures are shown in the documents?\",\n",
    "        \"Describe any architectural illustrations or charts\",\n",
    "        \"What visual elements help explain the concepts?\",\n",
    "        \"Are there any mathematical formulas or equations shown?\"\n",
    "    ]\n",
    "    \n",
    "    for question in visual_questions:\n",
    "        try:\n",
    "            result = self.query(question)\n",
    "            print(f\"\\nâ“ {question}\")\n",
    "            print(f\"ğŸ’¡ Answer: {result['answer']}\")\n",
    "            \n",
    "            # Show if any image sources were used\n",
    "            sources = result['source_documents']\n",
    "            image_sources = [s for s in sources if s.metadata.get('type', '').endswith('_Image')]\n",
    "            if image_sources:\n",
    "                print(f\"ğŸ–¼ï¸ Used {len(image_sources)} image descriptions in answer\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… Multi-Modal RAG demonstration complete!\")\n",
    "\n",
    "def extract_images_from_pdf(self, pdf_path):\n",
    "    \"\"\"Extract images from PDF and convert to base64.\"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        \n",
    "        print(f\"ğŸ–¼ï¸  Extracting images from: {pdf_path.name}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            image_list = page.get_images()\n",
    "            \n",
    "            for img_index, img in enumerate(image_list):\n",
    "                try:\n",
    "                    # Get image data\n",
    "                    xref = img[0]\n",
    "                    pix = fitz.Pixmap(doc, xref)\n",
    "                    \n",
    "                    if pix.n - pix.alpha < 4:  # GRAY or RGB\n",
    "                        # Convert to PNG bytes\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        \n",
    "                        # Convert to base64\n",
    "                        img_base64 = base64.b64encode(img_data).decode()\n",
    "                        \n",
    "                        images.append({\n",
    "                            \"base64\": img_base64,\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"index\": img_index,\n",
    "                            \"source\": pdf_path.name\n",
    "                        })\n",
    "                    \n",
    "                    pix = None  # Free memory\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"   âœ… Extracted {len(images)} images\")\n",
    "        return images\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyMuPDF not installed. Install with: pip install PyMuPDF\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error extracting images from {pdf_path.name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def describe_image(self, image_base64, source_info):\n",
    "    \"\"\"Generate description of image using GPT-4o Vision.\"\"\"\n",
    "    try:\n",
    "        print(f\"   ğŸ” Analyzing image from {source_info}...\")\n",
    "        \n",
    "        # Use the latest OpenAI format for vision\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in detail. Focus on any text, diagrams, charts, figures, tables, or important visual elements that might be relevant for answering questions about the document. Include any mathematical formulas, architectural diagrams, or technical illustrations you see.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "                        \"detail\": \"high\"  # Use high detail for better analysis\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response = self.vision_llm.invoke([message])\n",
    "        description = response.content\n",
    "        \n",
    "        print(f\"   âœ… Generated description ({len(description)} chars)\")\n",
    "        return description\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error describing image: {e}\")\n",
    "        return f\"Image from {source_info} (description failed)\"\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_multimodal_rag = demo_multimodal_rag\n",
    "ComprehensiveRAGDemo.extract_images_from_pdf = extract_images_from_pdf\n",
    "ComprehensiveRAGDemo.describe_image = describe_image\n",
    "\n",
    "print(\"âœ… Multi-Modal RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All Multi-Modal RAG methods added!\n"
     ]
    }
   ],
   "source": [
    "# Add the remaining multi-modal methods\n",
    "def load_pdf_with_images(self, file_path):\n",
    "    \"\"\"Load PDF with both text and images.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Load text content\n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        \n",
    "        print(f\"ğŸ“„ Loading PDF text: {file_path.name}\")\n",
    "        loader = PyPDFLoader(str(file_path))\n",
    "        text_docs = loader.load()\n",
    "        \n",
    "        for doc in text_docs:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        documents.extend(text_docs)\n",
    "        print(f\"   âœ… Loaded {len(text_docs)} text pages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading PDF text: {e}\")\n",
    "    \n",
    "    # Extract and process images\n",
    "    images = self.extract_images_from_pdf(file_path)\n",
    "    \n",
    "    for img in images:\n",
    "        # Generate description\n",
    "        description = self.describe_image(\n",
    "            img[\"base64\"], \n",
    "            f\"{img['source']} (page {img['page']})\"\n",
    "        )\n",
    "        \n",
    "        # Create document for image description\n",
    "        img_doc = Document(\n",
    "            page_content=f\"Image from page {img['page']}: {description}\",\n",
    "            metadata={\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Image\",\n",
    "                \"content_type\": \"image\",\n",
    "                \"page\": img[\"page\"],\n",
    "                \"image_base64\": img[\"base64\"]  # Store for potential display\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        documents.append(img_doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_documents_with_images(self, folder_path=\"EmbeddingDocs\"):\n",
    "    \"\"\"Load all documents with multi-modal processing.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"âŒ Folder {folder_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ğŸ“ Loading documents with multi-modal processing from: {folder}\")\n",
    "    all_documents = []\n",
    "    \n",
    "    # Find files\n",
    "    pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "    word_files = list(folder.glob(\"*.docx\")) + list(folder.glob(\"*.doc\"))\n",
    "    \n",
    "    print(f\"   Found {len(pdf_files)} PDF files and {len(word_files)} Word files\")\n",
    "    \n",
    "    # Process PDF files with images\n",
    "    for pdf_file in pdf_files:\n",
    "        docs = self.load_pdf_with_images(pdf_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    # Process Word files\n",
    "    for word_file in word_files:\n",
    "        docs = self.load_word_document(word_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    self.processed_content = all_documents\n",
    "    print(f\"ğŸ“š Total content pieces loaded: {len(all_documents)}\")\n",
    "    \n",
    "    # Show content breakdown\n",
    "    content_types = {}\n",
    "    for doc in all_documents:\n",
    "        content_type = doc.metadata.get('type', 'Unknown')\n",
    "        content_types[content_type] = content_types.get(content_type, 0) + 1\n",
    "    \n",
    "    print(\"ğŸ“Š Content breakdown:\")\n",
    "    for content_type, count in content_types.items():\n",
    "        print(f\"   {content_type}: {count}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def create_multimodal_knowledge_base(self, documents=None):\n",
    "    \"\"\"Create vector store from multi-modal documents.\"\"\"\n",
    "    if documents is None:\n",
    "        documents = self.processed_content\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"âŒ No documents to process\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ”ª Processing {len(documents)} content pieces...\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"ğŸ“„ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"ğŸ§® Creating embeddings for multi-modal content...\")\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Multi-modal knowledge base created!\")\n",
    "\n",
    "# Add remaining methods to our class\n",
    "ComprehensiveRAGDemo.load_pdf_with_images = load_pdf_with_images\n",
    "ComprehensiveRAGDemo.load_documents_with_images = load_documents_with_images\n",
    "ComprehensiveRAGDemo.create_multimodal_knowledge_base = create_multimodal_knowledge_base\n",
    "\n",
    "print(\"âœ… All Multi-Modal RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ–¼ï¸ SECTION 3: MULTI-MODAL RAG DEMO\n",
      "============================================================\n",
      "ğŸ“ Loading documents with multi-modal processing from: EmbeddingDocs\n",
      "   Found 1 PDF files and 1 Word files\n",
      "ğŸ“„ Loading PDF text: Attention is All You Need.pdf\n",
      "   âœ… Loaded 15 text pages\n",
      "ğŸ–¼ï¸  Extracting images from: Attention is All You Need.pdf\n",
      "   âœ… Extracted 3 images\n",
      "   ğŸ” Analyzing image from Attention is All You Need.pdf (page 3)...\n",
      "   âœ… Generated description (2081 chars)\n",
      "   ğŸ” Analyzing image from Attention is All You Need.pdf (page 4)...\n",
      "   âœ… Generated description (1330 chars)\n",
      "   ğŸ” Analyzing image from Attention is All You Need.pdf (page 4)...\n",
      "   âœ… Generated description (1560 chars)\n",
      "ğŸ“„ Loading Word document: sample boomi dd (1).docx\n",
      "   âœ… Loaded Word document\n",
      "ğŸ“š Total content pieces loaded: 19\n",
      "ğŸ“Š Content breakdown:\n",
      "   PDF_Text: 15\n",
      "   PDF_Image: 3\n",
      "   Word_Text: 1\n",
      "ğŸ”ª Processing 19 content pieces...\n",
      "ğŸ“„ Created 71 chunks\n",
      "ğŸ§® Creating embeddings for multi-modal content...\n",
      "âœ… Multi-modal knowledge base created!\n",
      "\n",
      "ğŸª Multi-Modal RAG Demo Questions:\n",
      "\n",
      "â“ What diagrams or figures are shown in the documents?\n",
      "ğŸ’¡ Answer: In the documents provided, there are two types of diagrams or figures shown:\n",
      "\n",
      "1. **Flow Diagram:**\n",
      "   - The document includes a flow diagram under the \"Design\" section (2.1). This flow diagram likely illustrates the process flow of the integration technical design related to reservation jobs.\n",
      "\n",
      "2. **Sequence Diagram:**\n",
      "   - There is a sequence diagram mentioned under the \"Process Details\" section (4.1.1). This sequence diagram probably details the sequence of steps or interactions involved in the \"rp Reservation to Salesforce\" process.\n",
      "\n",
      "These are the specific diagrams or figures mentioned in the documents.\n",
      "ğŸ–¼ï¸ Used 2 image descriptions in answer\n",
      "\n",
      "â“ Describe any architectural illustrations or charts\n",
      "ğŸ’¡ Answer: One architectural illustration or chart that can be described is a diagram illustrating the architecture of a multi-head attention mechanism. In this diagram, there are three inputs labeled as \\( V \\), \\( K \\), and \\( Q \\) at the bottom, representing Value, Key, and Query matrices. Each input goes through a linear transformation. After the transformation, the inputs are fed into a \"Scaled Dot-Product Attention\" block, which is repeated multiple times (indicated by \\( h \\)), suggesting multiple attention heads in a multi-head attention mechanism.\n",
      "ğŸ–¼ï¸ Used 1 image descriptions in answer\n",
      "\n",
      "â“ What visual elements help explain the concepts?\n",
      "ğŸ’¡ Answer: The visual elements that help explain the concepts in the attention mechanism diagram include:\n",
      "- Boxes representing operations like \"Linear\", \"Concat\", and \"Scaled Dot-Product Attention\".\n",
      "- Arrows indicating the flow of data between these operations.\n",
      "- Color-coding for distinction: \"Scaled Dot-Product Attention\" in purple, \"Concat\" in yellow, and linear layers in light gray.\n",
      "\n",
      "These elements visually represent the flow and transformation of data through the multi-head attention mechanism, aiding in understanding complex dependencies in input sequences.\n",
      "ğŸ–¼ï¸ Used 1 image descriptions in answer\n",
      "\n",
      "â“ Are there any mathematical formulas or equations shown?\n",
      "ğŸ’¡ Answer: Yes, there is a mathematical formula presented in the context. The formula is for the attention mechanism and is denoted as:\n",
      "\n",
      "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
      "\n",
      "This formula is part of the Scaled Dot-Product Attention mechanism used in transformer models.\n",
      "ğŸ–¼ï¸ Used 2 image descriptions in answer\n",
      "\n",
      "âœ… Multi-Modal RAG demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Run Multi-Modal RAG Demo\n",
    "rag_demo.demo_multimodal_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Multi-Modal RAG Breakthroughs:\n",
    "\n",
    "1. **Image Extraction**: Automatically finds images in PDFs\n",
    "2. **Vision AI**: GPT-4o describes visual content in detail\n",
    "3. **Unified Search**: Text and image descriptions in same vector space\n",
    "4. **Visual Understanding**: Can answer questions about diagrams, charts\n",
    "5. **Future-Ready**: Cutting-edge AI capabilities\n",
    "\n",
    "**Ask about visual elements in your documents!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-modal capabilities\n",
    "visual_questions = [\n",
    "    \"What diagrams are shown in the Transformer paper?\",\n",
    "    \"Describe the architecture illustrations\",\n",
    "    \"What visual elements help explain the concepts?\",\n",
    "    \"Are there any mathematical formulas shown in images?\"\n",
    "]\n",
    "\n",
    "for q in visual_questions:\n",
    "    try:\n",
    "        result = rag_demo.query(q)\n",
    "        print(f\"\\nâ“ {q}\")\n",
    "        print(f\"ğŸ’¡ {result['answer'][:250]}...\")\n",
    "        \n",
    "        # Check if image sources were used\n",
    "        sources = result['source_documents']\n",
    "        image_sources = [s for s in sources if 'Image' in s.metadata.get('type', '')]\n",
    "        if image_sources:\n",
    "            print(f\"ğŸ–¼ï¸ Used {len(image_sources)} image descriptions!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ Comparison and Summary\n",
    "\n",
    "## RAG Evolution Comparison:\n",
    "\n",
    "| Feature | Basic Text RAG | Document RAG | Multi-Modal RAG |\n",
    "|---------|---------------|--------------|----------------|\n",
    "| **Data Source** | Sample text | PDF/Word files | Text + Images |\n",
    "| **Processing** | Simple chunking | Document parsing | Vision AI analysis |\n",
    "| **Understanding** | Text only | Text only | Text + Visual |\n",
    "| **Use Cases** | Simple Q&A | Document search | Complex analysis |\n",
    "| **Accuracy** | Good | Better | Best |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "\n",
    "## ğŸ“ Learning Outcomes:\n",
    "\n",
    "âœ… **Understood RAG fundamentals** - retrieval + generation  \n",
    "âœ… **Experienced document processing** - real-world file handling  \n",
    "âœ… **Explored cutting-edge AI** - multi-modal capabilities  \n",
    "âœ… **Saw practical applications** - company knowledge bases  \n",
    "âœ… **Learned latest techniques** - GPT-4o vision integration  \n",
    "\n",
    "## ğŸš€ Next Steps for Interns:\n",
    "\n",
    "1. **Experiment** with different document types\n",
    "2. **Try different chunk sizes** and see the impact\n",
    "3. **Add metadata filtering** for more precise search\n",
    "4. **Build a web interface** using Streamlit or Flask\n",
    "5. **Implement evaluation metrics** to measure quality\n",
    "6. **Explore other embedding models** and compare results\n",
    "\n",
    "## ğŸ’¡ Real-World Applications:\n",
    "\n",
    "- ğŸ¢ **Internal Knowledge Bases**: Company policies, procedures\n",
    "- ğŸ“ **Customer Support**: Product documentation, troubleshooting\n",
    "- ğŸ“š **Research Assistance**: Academic papers, technical reports\n",
    "- ğŸ”§ **Developer Tools**: Code documentation, API references\n",
    "- ğŸ“ **Educational Platforms**: Course materials, study guides\n",
    "- ğŸ¥ **Healthcare**: Medical literature, diagnostic aids\n",
    "- âš–ï¸ **Legal**: Case law, contract analysis\n",
    "\n",
    "**Congratulations! You've mastered the complete RAG spectrum!** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
