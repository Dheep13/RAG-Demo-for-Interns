{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive RAG Demo - All Methods\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **three different RAG approaches** in a progressive learning format:\n",
    "\n",
    "1. **üìö Basic Text RAG**: Simple embedding with sample company data\n",
    "2. **üìÑ Document RAG**: Processing real PDF and Word documents\n",
    "3. **üñºÔ∏è Multi-Modal RAG**: Text + Images using GPT-4o Vision\n",
    "\n",
    "Perfect for intern training and understanding RAG evolution!\n",
    "\n",
    "### What You'll Learn:\n",
    "- ‚úÖ Core RAG concepts and implementation\n",
    "- ‚úÖ Document processing techniques\n",
    "- ‚úÖ Multi-modal AI capabilities\n",
    "- ‚úÖ Real-world applications\n",
    "- ‚úÖ Latest OpenAI Vision API usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-chroma langchain-community\n",
    "!pip install tiktoken chromadb pypdf docx2txt PyMuPDF\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "def load_env_file():\n",
    "    \"\"\"Load environment variables from .env file\"\"\"\n",
    "    code_snippets_path = Path(\"Code Snippets\")\n",
    "    env_file = code_snippets_path / \".env\"\n",
    "    \n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if '=' in line and not line.startswith('#'):\n",
    "                    key, value = line.strip().split('=', 1)\n",
    "                    value = value.strip(\"'\\\"\")\n",
    "                    os.environ[key] = value\n",
    "        print(\"‚úÖ Environment variables loaded from .env file\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå .env file not found\")\n",
    "        return False\n",
    "\n",
    "# Load the environment variables\n",
    "load_env_file()\n",
    "\n",
    "# Check if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚úÖ OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå OpenAI API key not found. Please check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è ComprehensiveRAGDemo Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveRAGDemo:\n",
    "    \"\"\"\n",
    "    Comprehensive RAG implementation with multiple approaches:\n",
    "    1. Basic text RAG with sample data\n",
    "    2. Document RAG with PDF/Word files\n",
    "    3. Multi-modal RAG with text + images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize comprehensive RAG system.\"\"\"\n",
    "        print(\"üöÄ Initializing Comprehensive RAG Demo...\")\n",
    "        \n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        # Use the latest vision model\n",
    "        self.vision_llm = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "        \n",
    "        # Text splitter\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.qa_chain = None\n",
    "        self.processed_content = []\n",
    "        \n",
    "        print(\"‚úÖ Comprehensive RAG Demo initialized!\")\n",
    "        print(\"üîß Supports: Basic Text, Document Processing, Multi-Modal\")\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question using the current RAG setup.\"\"\"\n",
    "        if self.qa_chain is None:\n",
    "            raise ValueError(\"No knowledge base created!\")\n",
    "        \n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"source_documents\": result[\"source_documents\"]\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ ComprehensiveRAGDemo class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the comprehensive RAG demo\n",
    "rag_demo = ComprehensiveRAGDemo()\n",
    "\n",
    "print(\"üéØ RAG system ready with:\")\n",
    "print(\"   üìä OpenAI Embeddings (ada-002)\")\n",
    "print(\"   ü§ñ GPT-3.5-turbo for text generation\")\n",
    "print(\"   üëÅÔ∏è GPT-4o for vision analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö SECTION 1: Basic Text RAG\n",
    "\n",
    "Let's start with the fundamentals - basic text embedding and retrieval using sample company data.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Understand document chunking\n",
    "- See embedding creation\n",
    "- Experience vector similarity search\n",
    "- Learn answer generation with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Basic Text RAG methods to our class\n",
    "def demo_basic_text_rag(self):\n",
    "    \"\"\"Demonstrate basic RAG with sample text data.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìö SECTION 1: BASIC TEXT RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample company documents\n",
    "    sample_docs = [\n",
    "        \"\"\"\n",
    "        Company Policy: Remote Work Guidelines\n",
    "        \n",
    "        Our company supports flexible remote work arrangements. Employees can work from home \n",
    "        up to 3 days per week with manager approval. Remote work days must be scheduled in advance.\n",
    "        \n",
    "        Equipment: The company provides laptops and necessary software for remote work.\n",
    "        Communication: Daily check-ins via Slack are required for remote workers.\n",
    "        Productivity: Remote workers must maintain the same productivity standards as office workers.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Employee Benefits Overview\n",
    "        \n",
    "        Health Insurance: Full medical, dental, and vision coverage provided.\n",
    "        Vacation Policy: 20 days of paid vacation per year, plus 10 sick days.\n",
    "        Professional Development: $2000 annual budget for training and conferences.\n",
    "        Retirement: 401k with 4% company matching.\n",
    "        Wellness: Free gym membership and mental health support.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        IT Security Guidelines\n",
    "        \n",
    "        Password Requirements: Minimum 12 characters with special characters.\n",
    "        VPN: Required for all remote connections to company systems.\n",
    "        Software Updates: Automatic updates must be enabled on all devices.\n",
    "        Data Protection: No company data on personal devices without encryption.\n",
    "        Incident Reporting: Security incidents must be reported within 1 hour.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"\"\"\n",
    "        Meeting Room Booking System\n",
    "        \n",
    "        Conference rooms can be booked through the company portal.\n",
    "        Maximum booking duration: 4 hours per session.\n",
    "        Cancellation: Must cancel at least 2 hours in advance.\n",
    "        Equipment: All rooms have projectors, whiteboards, and video conferencing.\n",
    "        Catering: Can be arranged through HR for meetings over 2 hours.\n",
    "        \"\"\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìÑ Processing {len(sample_docs)} sample documents...\")\n",
    "    \n",
    "    # Convert to Document objects\n",
    "    documents = [Document(page_content=doc) for doc in sample_docs]\n",
    "    \n",
    "    # Create knowledge base\n",
    "    self._create_basic_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions\n",
    "    demo_questions = [\n",
    "        \"How many days can I work from home?\",\n",
    "        \"What's our vacation policy?\",\n",
    "        \"What are the password requirements?\",\n",
    "        \"How do I book a meeting room?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüé™ Basic RAG Demo Questions:\")\n",
    "    for i, question in enumerate(demo_questions, 1):\n",
    "        print(f\"\\n{i}. {question}\")\n",
    "        result = self.query(question)\n",
    "        print(f\"üí° Answer: {result['answer']}\")\n",
    "        print(f\"üìñ Sources: {len(result['source_documents'])} chunks\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Basic Text RAG demonstration complete!\")\n",
    "\n",
    "def _create_basic_knowledge_base(self, documents):\n",
    "    \"\"\"Create knowledge base from basic documents.\"\"\"\n",
    "    # Split documents\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"üî™ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Basic knowledge base created!\")\n",
    "    \n",
    "    # Show embedding details for educational purposes\n",
    "    self._show_embedding_details(chunks[:2])  # Show first 2 chunks\n",
    "\n",
    "def _show_embedding_details(self, sample_chunks):\n",
    "    \"\"\"Show embedding details for educational purposes.\"\"\"\n",
    "    print(\"\\nüîç EMBEDDING DETAILS (Educational):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, chunk in enumerate(sample_chunks, 1):\n",
    "        print(f\"\\nüìÑ Chunk {i}:\")\n",
    "        print(f\"Text: {chunk.page_content[:100]}...\")\n",
    "        print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "        \n",
    "        # Generate embedding for this chunk\n",
    "        try:\n",
    "            embedding = self.embeddings.embed_query(chunk.page_content)\n",
    "            print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "            print(f\"Embedding type: {type(embedding)}\")\n",
    "            print(f\"First 10 values: {embedding[:10]}\")\n",
    "            print(f\"Embedding range: [{min(embedding):.4f}, {max(embedding):.4f}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "    \n",
    "    print(\"\\nüí° Key Points:\")\n",
    "    print(\"   ‚Ä¢ Each text chunk becomes a 1536-dimensional vector\")\n",
    "    print(\"   ‚Ä¢ Similar texts have similar embeddings (cosine similarity)\")\n",
    "    print(\"   ‚Ä¢ Vector database enables fast similarity search\")\n",
    "\n",
    "def query_database_directly(self, query_text, k=3):\n",
    "    \"\"\"Query the vector database directly and show similarity scores.\"\"\"\n",
    "    if self.vectorstore is None:\n",
    "        print(\"‚ùå No vector database available. Create knowledge base first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüîç DIRECT DATABASE QUERY: '{query_text}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = self.embeddings.embed_query(query_text)\n",
    "    print(f\"Query embedding dimensions: {len(query_embedding)}\")\n",
    "    print(f\"Query embedding preview: {query_embedding[:5]}...\")\n",
    "    \n",
    "    # Search with similarity scores\n",
    "    try:\n",
    "        # Use similarity_search_with_score for detailed results\n",
    "        results = self.vectorstore.similarity_search_with_score(query_text, k=k)\n",
    "        \n",
    "        print(f\"\\nüìä Top {k} Similar Chunks:\")\n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Similarity Score: {score:.4f}\")\n",
    "            print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            print(f\"   Type: {doc.metadata.get('type', 'Unknown')}\")\n",
    "            print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "        \n",
    "        print(\"\\nüí° Understanding Similarity Scores:\")\n",
    "        print(\"   ‚Ä¢ Lower scores = more similar (distance-based)\")\n",
    "        print(\"   ‚Ä¢ Scores typically range from 0.0 to 2.0\")\n",
    "        print(\"   ‚Ä¢ Score < 0.5: Very similar\")\n",
    "        print(\"   ‚Ä¢ Score 0.5-1.0: Moderately similar\")\n",
    "        print(\"   ‚Ä¢ Score > 1.0: Less similar\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying database: {e}\")\n",
    "        return None\n",
    "\n",
    "def show_vector_database_stats(self):\n",
    "    \"\"\"Show statistics about the vector database.\"\"\"\n",
    "    if self.vectorstore is None:\n",
    "        print(\"‚ùå No vector database available.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüìä VECTOR DATABASE STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Get collection info\n",
    "        collection = self.vectorstore._collection\n",
    "        count = collection.count()\n",
    "        \n",
    "        print(f\"Total vectors stored: {count}\")\n",
    "        print(f\"Embedding model: {self.embeddings.model}\")\n",
    "        print(f\"Vector dimensions: 1536 (OpenAI ada-002)\")\n",
    "        print(f\"Database type: ChromaDB (in-memory)\")\n",
    "        \n",
    "        # Sample a few vectors to show distribution\n",
    "        if count > 0:\n",
    "            sample_docs = self.vectorstore.similarity_search(\"sample\", k=min(3, count))\n",
    "            print(f\"\\nüìÑ Sample stored chunks:\")\n",
    "            for i, doc in enumerate(sample_docs, 1):\n",
    "                print(f\"   {i}. {doc.page_content[:80]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database stats: {e}\")\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_basic_text_rag = demo_basic_text_rag\n",
    "ComprehensiveRAGDemo._create_basic_knowledge_base = _create_basic_knowledge_base\n",
    "ComprehensiveRAGDemo._show_embedding_details = _show_embedding_details\n",
    "ComprehensiveRAGDemo.query_database_directly = query_database_directly\n",
    "ComprehensiveRAGDemo.show_vector_database_stats = show_vector_database_stats\n",
    "\n",
    "print(\"‚úÖ Basic Text RAG methods added with embedding visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Basic Text RAG Demo\n",
    "rag_demo.demo_basic_text_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Key Takeaways from Basic RAG:\n",
    "\n",
    "1. **Document Chunking**: Text is split into manageable pieces\n",
    "2. **Embeddings**: Each chunk becomes a vector representation\n",
    "3. **Vector Search**: Find most similar chunks to user question\n",
    "4. **Context Injection**: Relevant chunks are added to LLM prompt\n",
    "5. **Answer Generation**: LLM creates response based on context\n",
    "\n",
    "**Try your own questions about the company policies!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question with basic RAG\n",
    "question = \"What equipment does the company provide for remote work?\"\n",
    "\n",
    "result = rag_demo.query(question)\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"üí° Answer: {result['answer']}\")\n",
    "print(f\"üìñ Sources: {len(result['source_documents'])} chunks used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Deep Dive: Understanding Embeddings and Vector Search\n",
    "\n",
    "Let's explore what's happening under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vector database statistics\n",
    "rag_demo.show_vector_database_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database directly to see similarity scores\n",
    "query_text = \"remote work policy\"\n",
    "results = rag_demo.query_database_directly(query_text, k=3)\n",
    "\n",
    "print(\"\\nüéØ This shows you exactly how vector similarity search works!\")\n",
    "print(\"The RAG system uses these top chunks to generate the final answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different queries and their similarity scores\n",
    "test_queries = [\n",
    "    \"vacation days\",\n",
    "    \"password security\",\n",
    "    \"meeting room booking\",\n",
    "    \"completely unrelated topic\"\n",
    "]\n",
    "\n",
    "print(\"üß™ SIMILARITY COMPARISON TEST:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    results = rag_demo.query_database_directly(query, k=1)\n",
    "    if results:\n",
    "        best_score = results[0][1]\n",
    "        print(f\"   Best similarity score: {best_score:.4f}\")\n",
    "        if best_score < 0.5:\n",
    "            print(\"   ‚úÖ Excellent match found!\")\n",
    "        elif best_score < 1.0:\n",
    "            print(\"   ‚ö†Ô∏è Moderate match found\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Poor match - might hallucinate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Embedding Visualization Exercise\n",
    "\n",
    "Let's see how different texts create different embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings of similar vs different texts\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"Employees can work from home\",\n",
    "    \"Remote work is allowed for staff\",  # Similar meaning\n",
    "    \"The sky is blue today\",  # Completely different\n",
    "    \"Password must be 12 characters\"  # Different topic\n",
    "]\n",
    "\n",
    "print(\"üßÆ EMBEDDING SIMILARITY ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = rag_demo.embeddings.embed_query(text)\n",
    "    embeddings.append(embedding)\n",
    "    print(f\"\\nüìù Text: '{text}'\")\n",
    "    print(f\"   Embedding dimensions: {len(embedding)}\")\n",
    "    print(f\"   First 5 values: {embedding[:5]}\")\n",
    "\n",
    "# Calculate cosine similarities\n",
    "print(\"\\nüîó COSINE SIMILARITY MATRIX:\")\n",
    "print(\"(1.0 = identical, 0.0 = completely different)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "for i, text1 in enumerate(texts):\n",
    "    for j, text2 in enumerate(texts):\n",
    "        if i <= j:  # Only show upper triangle\n",
    "            similarity = similarity_matrix[i][j]\n",
    "            print(f\"'{text1[:20]}...' vs '{text2[:20]}...': {similarity:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Notice how similar meanings have higher cosine similarity!\")\n",
    "print(\"This is the foundation of semantic search in RAG systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìÑ SECTION 2: Document RAG\n",
    "\n",
    "Now let's process real documents from your EmbeddingDocs folder!\n",
    "\n",
    "### What's Different:\n",
    "- Real PDF and Word document processing\n",
    "- Automatic text extraction\n",
    "- Metadata preservation\n",
    "- Larger, more complex knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Document RAG methods to our class\n",
    "def demo_document_rag(self):\n",
    "    \"\"\"Demonstrate document RAG with PDF and Word files.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìÑ SECTION 2: DOCUMENT RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load documents from folder\n",
    "    documents = self.load_documents_from_folder()\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents found in EmbeddingDocs folder\")\n",
    "        return\n",
    "    \n",
    "    # Create knowledge base\n",
    "    self.create_document_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions based on loaded documents\n",
    "    print(\"\\nüé™ Document RAG Demo Questions:\")\n",
    "    \n",
    "    # Check what documents we have and ask relevant questions\n",
    "    doc_names = [doc.metadata.get('source', '') for doc in documents]\n",
    "    \n",
    "    if any('attention' in name.lower() for name in doc_names):\n",
    "        print(\"\\nüìÑ Questions about 'Attention is All You Need' paper:\")\n",
    "        questions = [\n",
    "            \"What is the Transformer architecture?\",\n",
    "            \"How does self-attention work?\",\n",
    "            \"What are the advantages of Transformers over RNNs?\"\n",
    "        ]\n",
    "        for q in questions:\n",
    "            result = self.query(q)\n",
    "            print(f\"‚ùì {q}\")\n",
    "            print(f\"üí° {result['answer'][:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    if any('boomi' in name.lower() for name in doc_names):\n",
    "        print(\"\\nüìÑ Questions about Boomi document:\")\n",
    "        questions = [\n",
    "            \"What is Boomi used for?\",\n",
    "            \"How does Boomi integration work?\"\n",
    "        ]\n",
    "        for q in questions:\n",
    "            result = self.query(q)\n",
    "            print(f\"‚ùì {q}\")\n",
    "            print(f\"üí° {result['answer'][:200]}...\")\n",
    "            print()\n",
    "    \n",
    "    print(\"‚úÖ Document RAG demonstration complete!\")\n",
    "\n",
    "def load_pdf_document(self, file_path):\n",
    "    \"\"\"Load a PDF document (text only).\"\"\"\n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        \n",
    "        print(f\"üìÑ Loading PDF: {file_path.name}\")\n",
    "        loader = PyPDFLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {len(documents)} pages\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_word_document(self, file_path):\n",
    "    \"\"\"Load a Word document.\"\"\"\n",
    "    try:\n",
    "        from langchain_community.document_loaders import Docx2txtLoader\n",
    "        \n",
    "        print(f\"üìÑ Loading Word document: {file_path.name}\")\n",
    "        loader = Docx2txtLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"Word_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded Word document\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Word document: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_from_folder(self, folder_path=\"EmbeddingDocs\"):\n",
    "    \"\"\"Load all documents from the specified folder.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder {folder_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÅ Loading documents from: {folder}\")\n",
    "    all_documents = []\n",
    "    \n",
    "    # Find files\n",
    "    pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "    word_files = list(folder.glob(\"*.docx\")) + list(folder.glob(\"*.doc\"))\n",
    "    \n",
    "    print(f\"   Found {len(pdf_files)} PDF files and {len(word_files)} Word files\")\n",
    "    \n",
    "    # Process PDF files\n",
    "    for pdf_file in pdf_files:\n",
    "        docs = self.load_pdf_document(pdf_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    # Process Word files\n",
    "    for word_file in word_files:\n",
    "        docs = self.load_word_document(word_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    self.processed_content = all_documents\n",
    "    print(f\"üìö Total documents loaded: {len(all_documents)}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def create_document_knowledge_base(self, documents):\n",
    "    \"\"\"Create knowledge base from documents.\"\"\"\n",
    "    print(f\"üî™ Processing {len(documents)} documents...\")\n",
    "    \n",
    "    # Split documents\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Document knowledge base created!\")\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_document_rag = demo_document_rag\n",
    "ComprehensiveRAGDemo.load_pdf_document = load_pdf_document\n",
    "ComprehensiveRAGDemo.load_word_document = load_word_document\n",
    "ComprehensiveRAGDemo.load_documents_from_folder = load_documents_from_folder\n",
    "ComprehensiveRAGDemo.create_document_knowledge_base = create_document_knowledge_base\n",
    "\n",
    "print(\"‚úÖ Document RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Document RAG Demo\n",
    "rag_demo.demo_document_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Document RAG Insights:\n",
    "\n",
    "1. **File Format Support**: PDFs, Word docs automatically processed\n",
    "2. **Text Extraction**: Handles complex document layouts\n",
    "3. **Metadata Tracking**: Source attribution for answers\n",
    "4. **Scalability**: Can handle hundreds of documents\n",
    "5. **Real-World Ready**: Production-ready document processing\n",
    "\n",
    "**Ask questions about your actual documents!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive questions about your documents\n",
    "questions = [\n",
    "    \"What is the main contribution of the Transformer architecture?\",\n",
    "    \"How does attention mechanism work?\",\n",
    "    \"What are the key components of Boomi integration?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    try:\n",
    "        result = rag_demo.query(q)\n",
    "        print(f\"\\n‚ùì {q}\")\n",
    "        print(f\"üí° {result['answer'][:300]}...\")\n",
    "        print(f\"üìñ Sources: {len(result['source_documents'])} document chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üñºÔ∏è SECTION 3: Multi-Modal RAG (Text + Images)\n",
    "\n",
    "The cutting-edge approach - understanding both text AND images!\n",
    "\n",
    "### Revolutionary Capabilities:\n",
    "- üñºÔ∏è Extract images from PDFs\n",
    "- üëÅÔ∏è AI-powered image description using GPT-4o\n",
    "- üß† Combined text + visual understanding\n",
    "- üìä Analyze charts, diagrams, and figures\n",
    "- üîç Search across both content types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Multi-Modal RAG methods to our class\n",
    "def demo_multimodal_rag(self):\n",
    "    \"\"\"Demonstrate multi-modal RAG with images.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üñºÔ∏è SECTION 3: MULTI-MODAL RAG DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load documents with images\n",
    "    documents = self.load_documents_with_images()\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents found in EmbeddingDocs folder\")\n",
    "        return\n",
    "    \n",
    "    # Create multi-modal knowledge base\n",
    "    self.create_multimodal_knowledge_base(documents)\n",
    "    \n",
    "    # Demo questions focusing on visual content\n",
    "    print(\"\\nüé™ Multi-Modal RAG Demo Questions:\")\n",
    "    \n",
    "    visual_questions = [\n",
    "        \"What diagrams or figures are shown in the documents?\",\n",
    "        \"Describe any architectural illustrations or charts\",\n",
    "        \"What visual elements help explain the concepts?\",\n",
    "        \"Are there any mathematical formulas or equations shown?\"\n",
    "    ]\n",
    "    \n",
    "    for question in visual_questions:\n",
    "        try:\n",
    "            result = self.query(question)\n",
    "            print(f\"\\n‚ùì {question}\")\n",
    "            print(f\"üí° Answer: {result['answer']}\")\n",
    "            \n",
    "            # Show if any image sources were used\n",
    "            sources = result['source_documents']\n",
    "            image_sources = [s for s in sources if s.metadata.get('type', '').endswith('_Image')]\n",
    "            if image_sources:\n",
    "                print(f\"üñºÔ∏è Used {len(image_sources)} image descriptions in answer\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Multi-Modal RAG demonstration complete!\")\n",
    "\n",
    "def extract_images_from_pdf(self, pdf_path):\n",
    "    \"\"\"Extract images from PDF and convert to base64.\"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        \n",
    "        print(f\"üñºÔ∏è  Extracting images from: {pdf_path.name}\")\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            image_list = page.get_images()\n",
    "            \n",
    "            for img_index, img in enumerate(image_list):\n",
    "                try:\n",
    "                    # Get image data\n",
    "                    xref = img[0]\n",
    "                    pix = fitz.Pixmap(doc, xref)\n",
    "                    \n",
    "                    if pix.n - pix.alpha < 4:  # GRAY or RGB\n",
    "                        # Convert to PNG bytes\n",
    "                        img_data = pix.tobytes(\"png\")\n",
    "                        \n",
    "                        # Convert to base64\n",
    "                        img_base64 = base64.b64encode(img_data).decode()\n",
    "                        \n",
    "                        images.append({\n",
    "                            \"base64\": img_base64,\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"index\": img_index,\n",
    "                            \"source\": pdf_path.name\n",
    "                        })\n",
    "                    \n",
    "                    pix = None  # Free memory\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error extracting image {img_index} from page {page_num + 1}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        doc.close()\n",
    "        print(f\"   ‚úÖ Extracted {len(images)} images\")\n",
    "        return images\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå PyMuPDF not installed. Install with: pip install PyMuPDF\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting images from {pdf_path.name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def describe_image(self, image_base64, source_info):\n",
    "    \"\"\"Generate description of image using GPT-4o Vision.\"\"\"\n",
    "    try:\n",
    "        print(f\"   üîç Analyzing image from {source_info}...\")\n",
    "        \n",
    "        # Use the latest OpenAI format for vision\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this image in detail. Focus on any text, diagrams, charts, figures, tables, or important visual elements that might be relevant for answering questions about the document. Include any mathematical formulas, architectural diagrams, or technical illustrations you see.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "                        \"detail\": \"high\"  # Use high detail for better analysis\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response = self.vision_llm.invoke([message])\n",
    "        description = response.content\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated description ({len(description)} chars)\")\n",
    "        return description\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error describing image: {e}\")\n",
    "        return f\"Image from {source_info} (description failed)\"\n",
    "\n",
    "# Add methods to our class\n",
    "ComprehensiveRAGDemo.demo_multimodal_rag = demo_multimodal_rag\n",
    "ComprehensiveRAGDemo.extract_images_from_pdf = extract_images_from_pdf\n",
    "ComprehensiveRAGDemo.describe_image = describe_image\n",
    "\n",
    "print(\"‚úÖ Multi-Modal RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the remaining multi-modal methods\n",
    "def load_pdf_with_images(self, file_path):\n",
    "    \"\"\"Load PDF with both text and images.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Load text content\n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        \n",
    "        print(f\"üìÑ Loading PDF text: {file_path.name}\")\n",
    "        loader = PyPDFLoader(str(file_path))\n",
    "        text_docs = loader.load()\n",
    "        \n",
    "        for doc in text_docs:\n",
    "            doc.metadata.update({\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Text\",\n",
    "                \"content_type\": \"text\"\n",
    "            })\n",
    "        \n",
    "        documents.extend(text_docs)\n",
    "        print(f\"   ‚úÖ Loaded {len(text_docs)} text pages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PDF text: {e}\")\n",
    "    \n",
    "    # Extract and process images\n",
    "    images = self.extract_images_from_pdf(file_path)\n",
    "    \n",
    "    for img in images:\n",
    "        # Generate description\n",
    "        description = self.describe_image(\n",
    "            img[\"base64\"], \n",
    "            f\"{img['source']} (page {img['page']})\"\n",
    "        )\n",
    "        \n",
    "        # Create document for image description\n",
    "        img_doc = Document(\n",
    "            page_content=f\"Image from page {img['page']}: {description}\",\n",
    "            metadata={\n",
    "                \"source\": file_path.name,\n",
    "                \"type\": \"PDF_Image\",\n",
    "                \"content_type\": \"image\",\n",
    "                \"page\": img[\"page\"],\n",
    "                \"image_base64\": img[\"base64\"]  # Store for potential display\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        documents.append(img_doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_documents_with_images(self, folder_path=\"EmbeddingDocs\"):\n",
    "    \"\"\"Load all documents with multi-modal processing.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    if not folder.exists():\n",
    "        print(f\"‚ùå Folder {folder_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÅ Loading documents with multi-modal processing from: {folder}\")\n",
    "    all_documents = []\n",
    "    \n",
    "    # Find files\n",
    "    pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "    word_files = list(folder.glob(\"*.docx\")) + list(folder.glob(\"*.doc\"))\n",
    "    \n",
    "    print(f\"   Found {len(pdf_files)} PDF files and {len(word_files)} Word files\")\n",
    "    \n",
    "    # Process PDF files with images\n",
    "    for pdf_file in pdf_files:\n",
    "        docs = self.load_pdf_with_images(pdf_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    # Process Word files\n",
    "    for word_file in word_files:\n",
    "        docs = self.load_word_document(word_file)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    self.processed_content = all_documents\n",
    "    print(f\"üìö Total content pieces loaded: {len(all_documents)}\")\n",
    "    \n",
    "    # Show content breakdown\n",
    "    content_types = {}\n",
    "    for doc in all_documents:\n",
    "        content_type = doc.metadata.get('type', 'Unknown')\n",
    "        content_types[content_type] = content_types.get(content_type, 0) + 1\n",
    "    \n",
    "    print(\"üìä Content breakdown:\")\n",
    "    for content_type, count in content_types.items():\n",
    "        print(f\"   {content_type}: {count}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def create_multimodal_knowledge_base(self, documents=None):\n",
    "    \"\"\"Create vector store from multi-modal documents.\"\"\"\n",
    "    if documents is None:\n",
    "        documents = self.processed_content\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"‚ùå No documents to process\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üî™ Processing {len(documents)} content pieces...\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = self.text_splitter.split_documents(documents)\n",
    "    print(f\"üìÑ Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"üßÆ Creating embeddings for multi-modal content...\")\n",
    "    self.vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=self.embeddings,\n",
    "        persist_directory=None\n",
    "    )\n",
    "    \n",
    "    # Create QA chain\n",
    "    self.qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=self.llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Multi-modal knowledge base created!\")\n",
    "\n",
    "# Add remaining methods to our class\n",
    "ComprehensiveRAGDemo.load_pdf_with_images = load_pdf_with_images\n",
    "ComprehensiveRAGDemo.load_documents_with_images = load_documents_with_images\n",
    "ComprehensiveRAGDemo.create_multimodal_knowledge_base = create_multimodal_knowledge_base\n",
    "\n",
    "print(\"‚úÖ All Multi-Modal RAG methods added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Multi-Modal RAG Demo\n",
    "rag_demo.demo_multimodal_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Multi-Modal RAG Breakthroughs:\n",
    "\n",
    "1. **Image Extraction**: Automatically finds images in PDFs\n",
    "2. **Vision AI**: GPT-4o describes visual content in detail\n",
    "3. **Unified Search**: Text and image descriptions in same vector space\n",
    "4. **Visual Understanding**: Can answer questions about diagrams, charts\n",
    "5. **Future-Ready**: Cutting-edge AI capabilities\n",
    "\n",
    "**Ask about visual elements in your documents!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-modal capabilities\n",
    "visual_questions = [\n",
    "    \"What diagrams are shown in the Transformer paper?\",\n",
    "    \"Describe the architecture illustrations\",\n",
    "    \"What visual elements help explain the concepts?\",\n",
    "    \"Are there any mathematical formulas shown in images?\"\n",
    "]\n",
    "\n",
    "for q in visual_questions:\n",
    "    try:\n",
    "        result = rag_demo.query(q)\n",
    "        print(f\"\\n‚ùì {q}\")\n",
    "        print(f\"üí° {result['answer'][:250]}...\")\n",
    "        \n",
    "        # Check if image sources were used\n",
    "        sources = result['source_documents']\n",
    "        image_sources = [s for s in sources if 'Image' in s.metadata.get('type', '')]\n",
    "        if image_sources:\n",
    "            print(f\"üñºÔ∏è Used {len(image_sources)} image descriptions!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Comparison and Summary\n",
    "\n",
    "## RAG Evolution Comparison:\n",
    "\n",
    "| Feature | Basic Text RAG | Document RAG | Multi-Modal RAG |\n",
    "|---------|---------------|--------------|----------------|\n",
    "| **Data Source** | Sample text | PDF/Word files | Text + Images |\n",
    "| **Processing** | Simple chunking | Document parsing | Vision AI analysis |\n",
    "| **Understanding** | Text only | Text only | Text + Visual |\n",
    "| **Use Cases** | Simple Q&A | Document search | Complex analysis |\n",
    "| **Accuracy** | Good | Better | Best |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "\n",
    "## üéì Learning Outcomes:\n",
    "\n",
    "‚úÖ **Understood RAG fundamentals** - retrieval + generation  \n",
    "‚úÖ **Experienced document processing** - real-world file handling  \n",
    "‚úÖ **Explored cutting-edge AI** - multi-modal capabilities  \n",
    "‚úÖ **Saw practical applications** - company knowledge bases  \n",
    "‚úÖ **Learned latest techniques** - GPT-4o vision integration  \n",
    "\n",
    "## üöÄ Next Steps for Interns:\n",
    "\n",
    "1. **Experiment** with different document types\n",
    "2. **Try different chunk sizes** and see the impact\n",
    "3. **Add metadata filtering** for more precise search\n",
    "4. **Build a web interface** using Streamlit or Flask\n",
    "5. **Implement evaluation metrics** to measure quality\n",
    "6. **Explore other embedding models** and compare results\n",
    "\n",
    "## üí° Real-World Applications:\n",
    "\n",
    "- üè¢ **Internal Knowledge Bases**: Company policies, procedures\n",
    "- üìû **Customer Support**: Product documentation, troubleshooting\n",
    "- üìö **Research Assistance**: Academic papers, technical reports\n",
    "- üîß **Developer Tools**: Code documentation, API references\n",
    "- üéì **Educational Platforms**: Course materials, study guides\n",
    "- üè• **Healthcare**: Medical literature, diagnostic aids\n",
    "- ‚öñÔ∏è **Legal**: Case law, contract analysis\n",
    "\n",
    "**Congratulations! You've mastered the complete RAG spectrum!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
